{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gym Install\n"
      ],
      "metadata": {
        "id": "FGZfT8FvEnsH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDakLucRL252",
        "outputId": "81346b50-f6b6-4f45-e705-13ce12416ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 137 kB/s \n",
            "\u001b[?25hCollecting swig==4.*\n",
            "  Downloading swig-4.1.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.11.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, pygame, box2d-py\n",
            "    Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.0\n"
          ]
        }
      ],
      "source": [
        "!bash pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZL584FSweKp",
        "outputId": "19b70e02-dad8-45b4-fd9a-0ac89a58b1c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  7 19:54:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    55W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AY8rlW-HtwIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5ca204-30d3-48e6-ec6a-a94318d5fa76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "from collections import deque\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from collections import deque\n",
        "from random import randrange\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils\n"
      ],
      "metadata": {
        "id": "5xox0mICExI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JBD9m8V4u_zK"
      },
      "outputs": [],
      "source": [
        "5\n",
        "\n",
        "def from_tuple_to_tensor(tuple_of_np):\n",
        "        tensor = np.zeros((len(tuple_of_np), tuple_of_np[0].shape[0], tuple_of_np[0].shape[1], tuple_of_np[0].shape[2]))\n",
        "        for i, x in enumerate(tuple_of_np):\n",
        "            tensor[i] = np.asarray(x)\n",
        "        return tensor\n",
        "        \n",
        "def device(force_cpu=True):\n",
        "    return \"cuda\" if torch.cuda.is_available() and not force_cpu else \"cpu\"                "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buffer\n"
      ],
      "metadata": {
        "id": "ip_QCfBFFC2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SumTree:\n",
        "    def __init__(self, size):\n",
        "        self.nodes = [0] * (2 * size - 1)\n",
        "        self.data = [0] * size\n",
        "\n",
        "        self.size = size\n",
        "        self.count = 0\n",
        "        self.real_size = 0\n",
        "\n",
        "    @property\n",
        "    def total(self):\n",
        "        return self.nodes[0]\n",
        "\n",
        "    def update(self, data_idx, value):\n",
        "        idx = data_idx + self.size - 1  \n",
        "        change = value - self.nodes[idx]\n",
        "\n",
        "        self.nodes[idx] = value\n",
        "\n",
        "        parent = (idx - 1) // 2\n",
        "        while parent >= 0:\n",
        "            self.nodes[parent] += change\n",
        "            parent = (parent - 1) // 2\n",
        "\n",
        "    def add(self, value, data):\n",
        "        self.data[self.count] = data\n",
        "        self.update(self.count, value)\n",
        "\n",
        "        self.count = (self.count + 1) % self.size\n",
        "        self.real_size = min(self.size, self.real_size + 1)\n",
        "\n",
        "    def get(self, cumsum):\n",
        "        assert cumsum <= self.total\n",
        "\n",
        "        idx = 0\n",
        "        while 2 * idx + 1 < len(self.nodes):\n",
        "            left, right = 2*idx + 1, 2*idx + 2\n",
        "\n",
        "            if cumsum <= self.nodes[left]:\n",
        "                idx = left\n",
        "            else:\n",
        "                idx = right\n",
        "                cumsum = cumsum - self.nodes[left]\n",
        "\n",
        "        data_idx = idx - self.size + 1\n",
        "\n",
        "        return data_idx, self.nodes[idx], self.data[data_idx]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SumTree(nodes={self.nodes.__repr__()}, data={self.data.__repr__()})\"\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, env, buffer_size, eps=1e-2, alpha=0.1, beta=0.1):\n",
        "        self.tree = SumTree(size=buffer_size)\n",
        "\n",
        "        # PER params\n",
        "        self.eps = eps  # minimal priority, prevents zero probabilities\n",
        "        self.alpha = alpha  # determines how much prioritization is used, α = 0 corresponding to the uniform case\n",
        "        self.beta = beta  # determines the amount of importance-sampling correction, b = 1 fully compensate for the non-uniform probabilities\n",
        "        self.max_priority = eps  # priority for new samples, init as eps\n",
        "\n",
        "        # transition: state, action, reward, next_state, done\n",
        "        self.state = torch.empty(buffer_size, env.observation_space._shape[0],env.observation_space._shape[1],env.observation_space._shape[2], dtype=torch.float)\n",
        "        self.action = torch.empty(buffer_size, dtype=torch.float)\n",
        "        self.reward = torch.empty(buffer_size, dtype=torch.float)\n",
        "        self.next_state = torch.empty(buffer_size, env.observation_space._shape[0],env.observation_space._shape[1],env.observation_space._shape[2], dtype=torch.float)\n",
        "        self.done = torch.empty(buffer_size, dtype=torch.int)\n",
        "\n",
        "        self.count = 0\n",
        "        self.real_size = 0\n",
        "        self.size = buffer_size\n",
        "\n",
        "    def add(self, state, action, reward, done, next_state):\n",
        "\n",
        "        # store transition index with maximum priority in sum tree\n",
        "        self.tree.add(self.max_priority, self.count)\n",
        "\n",
        "        # store transition in the buffer\n",
        "        self.state[self.count] = torch.as_tensor(state)\n",
        "        self.action[self.count] = torch.as_tensor(action)\n",
        "        self.reward[self.count] = torch.as_tensor(reward)\n",
        "        self.next_state[self.count] = torch.as_tensor(next_state)\n",
        "        self.done[self.count] = torch.as_tensor(done)\n",
        "\n",
        "        # update counters\n",
        "        self.count = (self.count + 1) % self.size\n",
        "        self.real_size = min(self.size, self.real_size + 1)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        assert self.real_size >= batch_size, \"buffer contains less samples than batch size\"\n",
        "\n",
        "        sample_idxs, tree_idxs = [], []\n",
        "        priorities = torch.empty(batch_size, 1, dtype=torch.float)\n",
        "\n",
        "        # To sample a minibatch of size k, the range [0, p_total] is divided equally into k ranges.\n",
        "        # Next, a value is uniformly sampled from each range. Finally the transitions that correspond\n",
        "        # to each of these sampled values are retrieved from the tree. (Appendix B.2.1, Proportional prioritization)\n",
        "        segment = self.tree.total / batch_size\n",
        "        for i in range(batch_size):\n",
        "            a, b = segment * i, segment * (i + 1)\n",
        "\n",
        "            cumsum = random.uniform(a, b)\n",
        "            # sample_idx is a sample index in buffer, needed further to sample actual transitions\n",
        "            # tree_idx is a index of a sample in the tree, needed further to update priorities\n",
        "            tree_idx, priority, sample_idx = self.tree.get(cumsum)\n",
        "\n",
        "            priorities[i] = priority\n",
        "            tree_idxs.append(tree_idx)\n",
        "            sample_idxs.append(sample_idx)\n",
        "\n",
        "        probs = priorities / self.tree.total\n",
        "        weights = (self.real_size * probs) ** -self.beta\n",
        "        weights = weights / weights.max()\n",
        "\n",
        "        batch = (\n",
        "            self.state[sample_idxs],\n",
        "            self.action[sample_idxs],\n",
        "            self.reward[sample_idxs],\n",
        "            self.done[sample_idxs],\n",
        "            self.next_state[sample_idxs]\n",
        "        )\n",
        "        return batch, weights, tree_idxs\n",
        "\n",
        "    def update_priorities(self, data_idxs, priorities):\n",
        "        if isinstance(priorities, torch.Tensor):\n",
        "            pass\n",
        "\n",
        "        for data_idx, priority in zip(data_idxs, priorities):\n",
        "            # The first variant we consider is the direct, proportional prioritization where p_i = |δ_i| + eps,\n",
        "            # where eps is a small positive constant that prevents the edge-case of transitions not being\n",
        "            # revisited once their error is zero. (Section 3.3)\n",
        "            priority = (priority + self.eps) ** self.alpha\n",
        "\n",
        "            self.tree.update(data_idx, priority)\n",
        "            self.max_priority = max(self.max_priority, priority)        "
      ],
      "metadata": {
        "id": "sjhpwVg4W3_Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q_NETWORK\n"
      ],
      "metadata": {
        "id": "7JI7W96RGWNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#with warnings.catch_warnings():\n",
        "#    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,n_inputs, n_outputs,hidden_size = 32,n_actions = 14, bias=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_frames = n_inputs\n",
        "        self.conv1 = nn.Conv2d(self.n_frames, hidden_size, 7)\n",
        "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 5)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutions\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Global Max Pooling\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape(batch_size, self.hidden_size, -1).max(axis=2).values\n",
        "\n",
        "        # Layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "\n",
        "        return x\n",
        "    \n",
        "\n",
        "    def to(self, device):\n",
        "        ret = super().to(device)\n",
        "        ret.device = device\n",
        "        return ret    \n",
        "\n",
        "\n",
        "class Q_network(nn.Module):\n",
        "\n",
        "    def __init__(self, env,  learning_rate=1e-4):\n",
        "        super(Q_network, self).__init__()\n",
        "\n",
        "        n_outputs = 14\n",
        "\n",
        "        #self.network = Net( ?? , ??)\n",
        "        self.network = Net( 1, n_outputs)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "    def greedy_action(self, state):\n",
        "        # greedy action = ??\n",
        "        # greedy_a = 0\n",
        "        qvals = self.get_qvals(state)\n",
        "        greedy_a = torch.max(qvals, dim=-1)[1].item()\n",
        "        return greedy_a\n",
        "\n",
        "    def get_qvals(self, state):\n",
        "        #out = ???\n",
        "        out = self.network.forward(state)\n",
        "        return out "
      ],
      "metadata": {
        "id": "PO0TJveyGUI-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy\n"
      ],
      "metadata": {
        "id": "pjnhJnX8GK3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTTKszTz5XXW",
        "outputId": "dbcfe921-7667-4389-b785-8533593a765d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Fa6JwYl9RybV"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    continuous = True\n",
        "\n",
        "    def __init__(self, env = gym.make('CarRacing-v2', continuous = True, render_mode=\"rgb_array\"), rew_thre = 400, learning_rate=0.1, initial_epsilon=0.9, batch_size= 64):\n",
        "\n",
        "        super(Policy, self).__init__()\n",
        "        self.env = env\n",
        "\n",
        "\n",
        "        self.network = Q_network(env, learning_rate)\n",
        "        self.target_network = deepcopy(self.network)\n",
        "        self.buffer = PrioritizedReplayBuffer(env, 50000)\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.window = 50\n",
        "        self.reward_threshold = rew_thre\n",
        "        self.step_count = 0\n",
        "        self.episode = 0\n",
        "        self.rewards = 0\n",
        "        self.update_loss = []\n",
        "        self.training_rewards = []\n",
        "        self.training_loss = []\n",
        "        self.mean_training_rewards = []\n",
        "        self.sync_eps = []\n",
        "\n",
        "        self.gs = transforms.Grayscale()\n",
        "        self.rs = transforms.Resize((64,64))\n",
        "\n",
        "        self.moves = np.array([[1,1,0],[0.5,1,0],[0.1,1,0],\n",
        "            [1,0.5,0],[0.5,0.5,0],[0.1,0.5,0],\n",
        "            [-1,1,0],[-0.5,1,0],[-0.1,1,0],\n",
        "            [-1,0.5,0],[-0.5,0.5,0],[-0.1,0.5,0],\n",
        "            [0,1,0],[0,0.5,0]])\n",
        "\n",
        "    def preproc_state(self, state):\n",
        "        # State Preprocessing\n",
        "        state = state[:83,:].transpose(2,0,1) #Torch wants images in format (channels, height, width)\n",
        "        state = torch.from_numpy(state).float()\n",
        "        state = self.gs(state) # grayscale\n",
        "        state = self.rs(state) # resize\n",
        "        return state/255 # normalize \n",
        "                \n",
        "    \n",
        "    def act(self, state):\n",
        "        state = self.preproc_state(state).unsqueeze(0)\n",
        "        return self.moves[self.network.greedy_action(state)]\n",
        "\n",
        "    def take_step(self, mode='exploit'):\n",
        "        # choose action with epsilon greedy\n",
        "        if mode == 'explore':\n",
        "                action = randrange(14)\n",
        "\n",
        "        else:\n",
        "                action = self.network.greedy_action(self.preproc_state(self.s_0).unsqueeze(0))\n",
        "\n",
        "\n",
        "        #simulate action\n",
        "        s_1, r, done, _= self.env.step(self.moves[action])\n",
        "\n",
        "        #put experience in the buffer\n",
        "    \n",
        "        self.buffer.add(self.s_0, action, r, done, s_1)\n",
        "\n",
        "        self.rewards += r\n",
        "\n",
        "        self.s_0 = s_1.copy()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if done:\n",
        "\n",
        "            self.s_0 = self.env.reset()\n",
        "        return done    \n",
        "\n",
        "    def train(self,\n",
        "     gamma=0.99, \n",
        "     max_episodes=500,\n",
        "     network_update_frequency=10,\n",
        "     network_sync_frequency=200):\n",
        "        \n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.s_0 = self.env.reset()\n",
        "\n",
        "        # Populate replay buffer\n",
        "        for i in range(1000):\n",
        "            self.take_step(mode='explore')\n",
        "        ep = 0\n",
        "        training = True\n",
        "        self.populate = False\n",
        "        while training:\n",
        "\n",
        "            self.s_0= self.env.reset()\n",
        "\n",
        "            self.rewards = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                if ((ep % 5) == 0):\n",
        "                    self.env.render()\n",
        "\n",
        "                p = np.random.random()\n",
        "                if p < self.epsilon:\n",
        "                    done = self.take_step(mode='explore')\n",
        "                    #print(\"explore\")\n",
        "                else:\n",
        "                    done = self.take_step(mode='exploit')\n",
        "                    #print(\"train\")\n",
        "                # Update network\n",
        "                if self.step_count % network_update_frequency == 0:\n",
        "                    self.update()\n",
        "                # Sync networks\n",
        "                if self.step_count % network_sync_frequency == 0:\n",
        "                    self.target_network.load_state_dict(\n",
        "                        self.network.state_dict())\n",
        "                    self.sync_eps.append(ep)\n",
        "\n",
        "                if done:\n",
        "                    if self.epsilon >= 0.05:\n",
        "                        self.epsilon = self.epsilon * 0.95\n",
        "                    ep += 1\n",
        "                    if self.rewards > 2000:\n",
        "                        self.training_rewards.append(2000)\n",
        "                    elif self.rewards > 1000:\n",
        "                        self.training_rewards.append(1000)\n",
        "                    elif self.rewards > 500:\n",
        "                        self.training_rewards.append(500)\n",
        "                    else:\n",
        "                        self.training_rewards.append(self.rewards)\n",
        "                    if len(self.update_loss) == 0:\n",
        "                        self.training_loss.append(0)\n",
        "                    else:\n",
        "                        self.training_loss.append(np.mean(self.update_loss))\n",
        "\n",
        "                    mean_rewards = np.mean(self.training_rewards[-self.window:])\n",
        "                    mean_loss = np.mean(self.training_loss[-self.window:])\n",
        "                    self.mean_training_rewards.append(mean_rewards)\n",
        "                    print(\n",
        "                        \"Episode {:d} Mean Rewards {:.2f}  Episode reward = {:.2f}   mean loss = {:.2f}\\n\".format(\n",
        "                            ep, mean_rewards, self.rewards, mean_loss), end=\"\")\n",
        "                    \n",
        "                    print(\"miao\")\n",
        "\n",
        "                    if ep >= max_episodes:\n",
        "                        training = False\n",
        "                        print('\\nEpisode limit reached.')\n",
        "                        break\n",
        "                    if mean_rewards >= self.reward_threshold:\n",
        "                        training = False\n",
        "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
        "                            ep))\n",
        "                        #break\n",
        "            self.save()            \n",
        "        # save models\n",
        "        self.save()\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.state_dict(), 'model.pt')\n",
        "        model_save_name = 'model.pt'\n",
        "        path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self):\n",
        "        self.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "    def calculate_loss(self, batch, weights=None):\n",
        "        #extract info from batch\n",
        "        states, actions, rewards, dones, next_states = list(batch)\n",
        "\n",
        "        #transform in torch tensors\n",
        "        rewards = torch.FloatTensor(rewards).reshape(-1, 1)\n",
        "        actions = torch.LongTensor(np.array(actions)).reshape(-1, 1)\n",
        "        dones = torch.IntTensor(dones).reshape(-1, 1)\n",
        "        states = from_tuple_to_tensor(states)\n",
        "        next_states = from_tuple_to_tensor(next_states)\n",
        "\n",
        "        states = torch.vstack([self.preproc_state(state).unsqueeze(0) for state in states])\n",
        "        qvals = self.network.get_qvals(states)\n",
        "        #states = torch.stack([self.preproc_state(state) for state in states]).unsequeeze(0)\n",
        "\n",
        "        qvals = torch.gather(qvals, 1, actions)\n",
        "\n",
        "        # target Q(s,a) = ??\n",
        "        next_states = torch.vstack([self.preproc_state(state).unsqueeze(0) for state in next_states])\n",
        "        next_qvals= self.network.get_qvals(next_states)\n",
        "        next_qvals_max = torch.max(next_qvals, dim=-1)[0].reshape(-1, 1)\n",
        "        target_qvals = rewards + (1 - dones)*self.gamma*next_qvals_max\n",
        "\n",
        "        loss = self.loss_function(qvals, target_qvals)\n",
        "\n",
        "        # loss = self.loss_function( Q(s,a) , target_Q(s,a))\n",
        "\n",
        "        if weights is None:\n",
        "            weights = torch.ones_like(qvals)\n",
        "\n",
        "        td_error = torch.abs(qvals - target_qvals).detach()\n",
        "        loss = torch.mean((qvals - target_qvals) ** 2 * weights)\n",
        "\n",
        "        return loss, td_error\n",
        "\n",
        "    def update(self):\n",
        "        self.network.optimizer.zero_grad()\n",
        "        batch, weights, tree_idxs = self.buffer.sample(batch_size=self.batch_size)\n",
        "        loss, td_error = self.calculate_loss(batch, weights = weights)\n",
        "\n",
        "        self.buffer.update_priorities(tree_idxs, td_error)\n",
        "\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        self.network.optimizer.step()\n",
        "\n",
        "        self.update_loss.append(loss.item())    \n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4eEL9USqK1uL"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    agent = Policy()\n",
        "    print(agent.network.network.device)\n",
        "    agent.train()\n",
        "    agent.save()\n",
        "\n",
        "def loadtrain():\n",
        "    agent = Policy()\n",
        "    agent.load()\n",
        "    agent.train()\n",
        "    agent.save()       \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fb2h15EUmNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca2a244-154f-4303-b548-3b905598c53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Episode 1 Mean Rewards -95.93  Episode reward = -95.93   mean loss = 0.94\n",
            "miao\n",
            "Episode 2 Mean Rewards -87.91  Episode reward = -79.90   mean loss = 6.34\n",
            "miao\n",
            "Episode 3 Mean Rewards -86.27  Episode reward = -82.99   mean loss = 12.02\n",
            "miao\n",
            "Episode 4 Mean Rewards -84.13  Episode reward = -77.71   mean loss = 14.25\n",
            "miao\n",
            "Episode 5 Mean Rewards -83.33  Episode reward = -80.13   mean loss = 14.97\n",
            "miao\n",
            "Episode 6 Mean Rewards -83.11  Episode reward = -82.01   mean loss = 15.13\n",
            "miao\n",
            "Episode 7 Mean Rewards -83.13  Episode reward = -83.22   mean loss = 15.01\n",
            "miao\n",
            "Episode 8 Mean Rewards -82.27  Episode reward = -76.27   mean loss = 14.73\n",
            "miao\n",
            "Episode 9 Mean Rewards -82.26  Episode reward = -82.21   mean loss = 14.33\n",
            "miao\n",
            "Episode 10 Mean Rewards -81.78  Episode reward = -77.44   mean loss = 13.92\n",
            "miao\n",
            "Episode 11 Mean Rewards -81.83  Episode reward = -82.33   mean loss = 13.53\n",
            "miao\n",
            "Episode 12 Mean Rewards -81.91  Episode reward = -82.82   mean loss = 13.14\n",
            "miao\n",
            "Episode 13 Mean Rewards -82.05  Episode reward = -83.66   mean loss = 12.75\n",
            "miao\n",
            "Episode 14 Mean Rewards -81.88  Episode reward = -79.76   mean loss = 12.40\n",
            "miao\n",
            "Episode 15 Mean Rewards -81.87  Episode reward = -81.68   mean loss = 12.07\n",
            "miao\n",
            "Episode 16 Mean Rewards -81.66  Episode reward = -78.42   mean loss = 11.78\n",
            "miao\n",
            "Episode 17 Mean Rewards -81.85  Episode reward = -84.89   mean loss = 11.50\n",
            "miao\n",
            "Episode 18 Mean Rewards -81.74  Episode reward = -79.87   mean loss = 11.23\n",
            "miao\n",
            "Episode 19 Mean Rewards -81.65  Episode reward = -80.16   mean loss = 10.98\n",
            "miao\n",
            "Episode 20 Mean Rewards -81.75  Episode reward = -83.50   mean loss = 10.74\n",
            "miao\n",
            "Episode 21 Mean Rewards -81.68  Episode reward = -80.31   mean loss = 10.51\n",
            "miao\n",
            "Episode 22 Mean Rewards -81.76  Episode reward = -83.44   mean loss = 10.30\n",
            "miao\n",
            "Episode 23 Mean Rewards -81.68  Episode reward = -80.00   mean loss = 10.10\n",
            "miao\n",
            "Episode 24 Mean Rewards -81.77  Episode reward = -83.71   mean loss = 9.91\n",
            "miao\n",
            "Episode 25 Mean Rewards -81.76  Episode reward = -81.68   mean loss = 9.73\n",
            "miao\n",
            "Episode 26 Mean Rewards -81.79  Episode reward = -82.58   mean loss = 9.55\n",
            "miao\n",
            "Episode 27 Mean Rewards -81.93  Episode reward = -85.34   mean loss = 9.38\n",
            "miao\n",
            "Episode 28 Mean Rewards -82.04  Episode reward = -85.21   mean loss = 9.22\n",
            "miao\n",
            "Episode 29 Mean Rewards -82.18  Episode reward = -86.06   mean loss = 9.06\n",
            "miao\n",
            "Episode 30 Mean Rewards -82.24  Episode reward = -83.92   mean loss = 8.91\n",
            "miao\n",
            "Episode 31 Mean Rewards -82.24  Episode reward = -82.27   mean loss = 8.77\n",
            "miao\n",
            "Episode 32 Mean Rewards -82.27  Episode reward = -83.05   mean loss = 8.64\n",
            "miao\n",
            "Episode 33 Mean Rewards -82.33  Episode reward = -84.44   mean loss = 8.51\n",
            "miao\n",
            "Episode 34 Mean Rewards -82.47  Episode reward = -86.89   mean loss = 8.38\n",
            "miao\n",
            "Episode 35 Mean Rewards -82.56  Episode reward = -85.92   mean loss = 8.26\n",
            "miao\n",
            "Episode 36 Mean Rewards -82.70  Episode reward = -87.30   mean loss = 8.14\n",
            "miao\n",
            "Episode 37 Mean Rewards -82.78  Episode reward = -85.77   mean loss = 8.03\n",
            "miao\n",
            "Episode 38 Mean Rewards -82.84  Episode reward = -85.24   mean loss = 7.92\n",
            "miao\n",
            "Episode 39 Mean Rewards -82.84  Episode reward = -82.76   mean loss = 7.82\n",
            "miao\n",
            "Episode 40 Mean Rewards -82.96  Episode reward = -87.73   mean loss = 7.72\n",
            "miao\n",
            "Episode 41 Mean Rewards -82.87  Episode reward = -79.08   mean loss = 7.62\n",
            "miao\n",
            "Episode 42 Mean Rewards -82.91  Episode reward = -84.79   mean loss = 7.52\n",
            "miao\n",
            "Episode 43 Mean Rewards -82.97  Episode reward = -85.45   mean loss = 7.43\n",
            "miao\n",
            "Episode 44 Mean Rewards -83.06  Episode reward = -86.75   mean loss = 7.34\n",
            "miao\n",
            "Episode 45 Mean Rewards -83.06  Episode reward = -83.22   mean loss = 7.26\n",
            "miao\n",
            "Episode 46 Mean Rewards -83.15  Episode reward = -87.26   mean loss = 7.17\n",
            "miao\n",
            "Episode 47 Mean Rewards -83.23  Episode reward = -86.71   mean loss = 7.09\n",
            "miao\n",
            "Episode 48 Mean Rewards -83.29  Episode reward = -86.16   mean loss = 7.01\n",
            "miao\n",
            "Episode 49 Mean Rewards -83.33  Episode reward = -85.35   mean loss = 6.93\n",
            "miao\n",
            "Episode 50 Mean Rewards -83.34  Episode reward = -83.82   mean loss = 6.86\n",
            "miao\n",
            "Episode 51 Mean Rewards -83.16  Episode reward = -86.93   mean loss = 6.90\n",
            "miao\n",
            "Episode 52 Mean Rewards -83.30  Episode reward = -86.93   mean loss = 6.72\n",
            "miao\n",
            "Episode 53 Mean Rewards -83.40  Episode reward = -87.77   mean loss = 6.32\n",
            "miao\n",
            "Episode 54 Mean Rewards -83.61  Episode reward = -88.47   mean loss = 5.96\n",
            "miao\n",
            "Episode 55 Mean Rewards -83.75  Episode reward = -87.01   mean loss = 5.66\n",
            "miao\n",
            "Episode 56 Mean Rewards -83.83  Episode reward = -85.92   mean loss = 5.40\n",
            "miao\n",
            "Episode 57 Mean Rewards -83.88  Episode reward = -85.71   mean loss = 5.17\n",
            "miao\n",
            "Episode 58 Mean Rewards -83.98  Episode reward = -81.48   mean loss = 4.97\n",
            "miao\n",
            "Episode 59 Mean Rewards -84.00  Episode reward = -82.82   mean loss = 4.80\n",
            "miao\n",
            "Episode 60 Mean Rewards -84.15  Episode reward = -85.35   mean loss = 4.65\n",
            "miao\n",
            "Episode 61 Mean Rewards -84.26  Episode reward = -87.42   mean loss = 4.51\n",
            "miao\n",
            "Episode 62 Mean Rewards -84.31  Episode reward = -85.56   mean loss = 4.38\n",
            "miao\n",
            "Episode 63 Mean Rewards -84.36  Episode reward = -86.16   mean loss = 4.27\n",
            "miao\n",
            "Episode 64 Mean Rewards -84.50  Episode reward = -86.97   mean loss = 4.17\n",
            "miao\n",
            "Episode 65 Mean Rewards -84.58  Episode reward = -85.56   mean loss = 4.07\n",
            "miao\n",
            "Episode 66 Mean Rewards -84.69  Episode reward = -83.74   mean loss = 3.97\n",
            "miao\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}